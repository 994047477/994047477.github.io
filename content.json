{"meta":{"title":"LiXuan's blog","subtitle":"Deeplearning|MachineLearning|ComputerVision","description":"Deeplearning|MachineLearning|ComputerVision","author":"leexuan","url":"http://yoursite.com"},"pages":[{"title":"404","date":"2018-10-24T07:55:18.000Z","updated":"2018-10-24T07:55:18.744Z","comments":true,"path":"404/index.html","permalink":"http://yoursite.com/404/index.html","excerpt":"","text":""},{"title":"about","date":"2019-08-02T01:52:51.396Z","updated":"2019-08-02T01:52:51.391Z","comments":true,"path":"about/index.html","permalink":"http://yoursite.com/about/index.html","excerpt":"","text":"ResumeSelf Introduction 我是leexuan，来自安徽芜湖，98年萌新，本科毕业于合肥工业大学计算机专业，在大学期间，大一好好学习天天向上，然后转到计算机专业，大二接触JAVA、程序设计竞赛、WEB开发等，大三学习python、爬虫技术、大数据技术、机器学习、深度学习，在大三暑假，获得推免资格，保研到复旦大学攻读硕士，大四开始接触计算机视觉的一些研究，大四接触的计算机视觉领域包括图像分类、目标检测、OCR、语义分割… 读研期间，我将会抽空去更新博客，将所学的一些知识(论文、书籍等)进行总结归纳。 如果你和我需要和我交流（包括竞赛、科研、编程开发、求职等）或者是博客里有些观点写的有误，可以通过邮箱联系我，请多多指教！ Contact Mail: 19212010022(AT)mail.fudan.edu.cnEducation Fudan University 2019.9 - 2022.6 M.S. in Software Engineering Research on computer vision and deeplearning Hefei University of Technology 2015.9 - 2019.6 B.S. in Computer Science GPA:3.75/4.3 , Rank: 7/196 Skills English – CET4,CET6 Python Computer Vision(Object Detection) Deep Learning Machine Learning Hadoop,Hive (simple big data processing ) Web Spider Simple website building (use JSP and Servlet) Linux/MacOS Data Mining pytorch/tensorflow/keras learning…. Honors &amp; Awards 全国高中数学联赛三等奖 全国大学生数学竞赛安徽赛区三等奖 安徽省程序设计竞赛二等奖 安徽省大数据技术与应用竞赛冠军 安徽省三创赛二等奖 合肥工业大学一等奖学金和三好学生等 合肥工业大学优秀毕业生,安徽省优秀毕业生 Finish OCR Graduation project — Use CTPN and Tesseract network TODO the implement of MachineLearning In Action read the Watermelon book and Statistical learning method"},{"title":"search","date":"2018-10-24T07:55:10.000Z","updated":"2018-10-24T07:55:10.868Z","comments":true,"path":"search/index.html","permalink":"http://yoursite.com/search/index.html","excerpt":"","text":""},{"title":"categories","date":"2019-01-04T08:32:33.441Z","updated":"2019-01-04T08:32:33.441Z","comments":true,"path":"categories/index.html","permalink":"http://yoursite.com/categories/index.html","excerpt":"","text":""},{"title":"tags","date":"2019-01-04T08:25:56.849Z","updated":"2019-01-04T08:25:56.849Z","comments":true,"path":"tags/index.html","permalink":"http://yoursite.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"Batch Normalization解析","slug":"Batch-Normalization","date":"2019-08-08T08:47:43.000Z","updated":"2019-08-09T02:51:45.517Z","comments":true,"path":"2019/08/08/Batch-Normalization/","link":"","permalink":"http://yoursite.com/2019/08/08/Batch-Normalization/","excerpt":"论文链接 : https://arxiv.org/abs/1502.03167","text":"论文链接 : https://arxiv.org/abs/1502.03167 介绍Batch Normalization 是批归一化，用来解决神经网络中协方差偏移问题，就是每一层的分布很混乱，除非从第一层开始每一层的输入的分布都很有效，否则就得不到好的效果。如下图所示。 所以，为了解决这个问题，我们引入了Batch Normalization. 介绍一下在机器学习中，是如何做feature scaling，因为输入的不同变量分布不同，比如说x1可能是代表房子的房间数量，x2代表房子的大小多少平方，那么x2和x1很显然不属于统一量纲，所以要做归一化，如下图所示。 原理顾名思义，Batch Normalization是对一个batch的数据做处理， 求均值 除以标准差 仿射变换（可选）（有时候需要把均值和方差调整到某些范围时可以加上） 然后将Batch Normalization的结果送给激活函数层，才能比较好的发挥激活函数的作用。 举个栗子，比如像Sigmoid函数，如果不做Batch Normalization，可能会导致输入分布在两边（就是不集中在[-1，1] ），梯度就很小，链式相乘之后就会接近于0，做了Batch Normalization，把输入集中在0附近，使得激活函数更好地发挥作用。 pytorch实现pytorch api1234567891011torch.nn.BatchNorm1d(num_features, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)torch.nn.BatchNorm2d(num_features, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) affine是仿射，track_running_stats是保留在训练时候的数据方差和均值，从而运用到测试中，因为在测试中是不方便计算方差和均值的。要注意在训练的时候选择model.train()模式在测试的时候选择model.eval()模式，这样Batch Normalization中的参数才会固定。 知乎上有个BN为什么能够解决梯度消失和爆炸问题的回答写的很好:https://www.zhihu.com/question/38102762/answer/391649040","categories":[],"tags":[{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://yoursite.com/tags/Deep-Learning/"}]},{"title":"经典CNN结构解析与实现","slug":"CNN-Network","date":"2019-08-02T01:36:03.000Z","updated":"2019-08-08T11:16:24.928Z","comments":true,"path":"2019/08/02/CNN-Network/","link":"","permalink":"http://yoursite.com/2019/08/02/CNN-Network/","excerpt":"实现代码地址：Github","text":"实现代码地址：Github 目录 AlexNet VGG Google Inception Net ResNet DenseNet MobileNet Shuffle Net AlexNet 首次使用CNN组建的深度网络在ImageNet上达到最好效果 它使用Relu 激活函数代替了传统的激活函数(Sigmoid/Tanh) 使用Dropout 解决过拟合问题 计算量较大 VGG VGG使用连续的几个3*3卷积核代替AlexNet中较大的卷积核，减少计算量。 VGG Net中采用的卷积核是非常一致的，全都是3*3的filter VGG 分为4-5个block，每个block包含若干个3*3的卷积层和一个2*2的MaxPooling层所以得到的conv4_3是16倍的下采样的feature map Inception Net 设计了Inception Block，采用了不同尺度的filter，size为1*1,3*3,5*5，获得了不同scale的特征，具有对不同尺度特征的提取能力，而且还通过1*1的卷积层来大大降低运算。 在卷积层结束后，使用全局AveragePooling，避免使用全连接层 ResNet 在VGG网络后，研究人员发现仅仅堆叠越来越多层的CNN 或 全连接层等是无法再提高准确率，主要原因就是梯度消失的问题，因为在多层神经网络中，梯度传到最前面的网络层就非常接近0了，导致前面网络层的参数更新很慢。于是提出ResNet去解决这个梯度消失的问题。 ResNet主要是设计了一个ResBlock。ResBlock使用skip connect 跳层连接，使得梯度能够有效地回传，从而参数得以更新，才敢设计比VGG深很多的网络。 DenseNet 设计思路与ResNet非常相似，但实际上一个很大的区别是，它是做多channel的特征的拼接，而ResNet是做求和计算 DenseNet主要包含两个模块，DenseBlock和Transition层，每个DenseBlock的内部是将第N层之前的feature map都经过卷积处理到固定shape拼接到第N层，Transition作为缓冲层是将维度进行降低，减少计算量 优点：梯度消失问题得到改善，特征复用，使用了multi-scale的特征,参数少，因为特征图的channel不像之前的那么多 MobileNet 解决深度学习网络移动设备上的运行速度问题，提出的高效的网络，在模型大小显著减少的情况下，对准确率不造成太大影响 使用 depthwise separable convolution 代替传统卷积网络。 depthwise separable convolution depthwise卷积 对输入的D*D*M的M个通道，每个通道使用一个filter进行卷积，即使用M个卷积核分别对M个通道进行卷积，得到feature mapD*D*M pointwise卷积 对经过上面步骤得到的feature mapD*D*M的每个cell(shape为 1*1*M) 使用1*1*M*N卷积核进行卷积 SqueezeNetShuffle Net","categories":[],"tags":[{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://yoursite.com/tags/Deep-Learning/"},{"name":"Image Classification","slug":"Image-Classification","permalink":"http://yoursite.com/tags/Image-Classification/"},{"name":"Feature Extract","slug":"Feature-Extract","permalink":"http://yoursite.com/tags/Feature-Extract/"}]},{"title":"《You Only Look Once》YOLOv1论文笔记","slug":"YOLOv1","date":"2019-08-01T08:25:01.000Z","updated":"2019-08-02T01:18:14.441Z","comments":true,"path":"2019/08/01/YOLOv1/","link":"","permalink":"http://yoursite.com/2019/08/01/YOLOv1/","excerpt":"YOLO v1论文:《You Only Look Once: Uniﬁed, Real-Time Object Detection》论文地址：http://www.arxiv.org/pdf/1506.02640.pdf","text":"YOLO v1论文:《You Only Look Once: Uniﬁed, Real-Time Object Detection》论文地址：http://www.arxiv.org/pdf/1506.02640.pdf 简介作者提出把目标检测任务当做回归任务来做，用一个神经网络来完成bbox(bounding box)以及class probabilities(类别概率)的回归，因为网络是单个完整的，不同于Faster R-CNN是分为RPN(Region Proposal Network,区域建议网络，用来生成一些proposal，输给后续的回归网络)和回归网络。所以YOLO网络是可以端到端的训练。 网络结构 YOLO系统流程 输入图片img resize到指定尺寸 通过预先设计好的多层CNN(一般包括特征抽取的VGG，以及回归层等) 根据回归层输出的坐标和置信度confidence来对proposal进行筛选 主要的网络结构如图如上图所示，输入一个图片，resize到448*448的，经过N多卷积网络层得到7*7*30的feature map 图片源于知乎用户这个维度30的意义如下图所示，包括confidence*2,(x1,y1)(x2,y2)四个坐标*2,20个类别概率(哪个概率大就是哪一类，用softmax实现)，每一个30维的vector代表一个7*7的feature map 的每一个cell里面的信息。 置信度在训练时计算如上图所示，Pr(object)就是选择一个cell中与训练样本ground truth的IOU大的那个proposal为1，另一个为0，所以这个置信度即为IOU或0。 Loss Function 如图所示，损失函数包括三部分：坐标回归损失、分类损失和置信度损失。坐标回归损失只是针对那些被认定为包含object的proposal才进行计算，对于没有检测到object的proposal不需要坐标回归的loss，分类损失也是检测到object的proposal要计算。还有一个置信度误差，不管有没有检测到object都要去计算。 Problem 每个vector的30维中只有一个20维的class probability ，但是有两个5维(4个坐标+1个置信度)向量，这代表了实际上7*7的feature map中的每一个单元都只能预测一个object，那么对于每个object为什么要使用两个回归向量呢？是为了精确地解决回归问题，最后这两个回归出来的proposal最多保留IOU大的那一个。","categories":[],"tags":[{"name":"Computer Vision","slug":"Computer-Vision","permalink":"http://yoursite.com/tags/Computer-Vision/"},{"name":"Object Detection","slug":"Object-Detection","permalink":"http://yoursite.com/tags/Object-Detection/"},{"name":"Notes","slug":"Notes","permalink":"http://yoursite.com/tags/Notes/"}]},{"title":"机器学习西瓜书第一章","slug":"ML_1","date":"2019-07-16T11:51:36.000Z","updated":"2019-07-17T02:20:12.202Z","comments":true,"path":"2019/07/16/ML_1/","link":"","permalink":"http://yoursite.com/2019/07/16/ML_1/","excerpt":"第一章 绪论归纳偏好 （挑选假设函数的基准） 一般根据“奥卡姆剃刀”原则，也就是选择最简单的假设函数。 如无必要，勿增实体。 比如说给定有限训练集，有很多假设都能使得结果与有限训练集标签相似，这时候需要一个准则来在这个庞大的假设空间中选择假设进行启发，就是奥卡姆剃刀准则。","text":"第一章 绪论归纳偏好 （挑选假设函数的基准） 一般根据“奥卡姆剃刀”原则，也就是选择最简单的假设函数。 如无必要，勿增实体。 比如说给定有限训练集，有很多假设都能使得结果与有限训练集标签相似，这时候需要一个准则来在这个庞大的假设空间中选择假设进行启发，就是奥卡姆剃刀准则。 没有归纳偏执或者归纳偏执太宽泛会导致 Overfitting，限制过大的归纳偏执也是有问题的，如果数据本身并不是线性的，强行用线性函数去做回归通常并不能得到好结果。 “”没有免费的午餐“ No Free Lunch Theorem定理: 总误差与学习算法无关，它们的期望性能相同。（NFL前提：所有“问题”出现的机会相同，或所有问题同等重要，但实际情形并不是这样） NFL定理最重要的寓意：脱离具体问题，空泛的谈论“什么学习算法更好”毫无意义。因为若考虑所有潜在的问题，则所有学习算法都一样好，要谈论算法的相对优劣，必须要针对具体的学习问题，在某些问题上表现好的学习算法，在另一些问题上却可能不尽如人意。学习算法自身的归纳偏好与问题是否相配，往往会起到决定性的作用。 参考: https://www.jianshu.com/p/b26b85fac9ad","categories":[],"tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"http://yoursite.com/tags/Machine-Learning/"},{"name":"西瓜书","slug":"西瓜书","permalink":"http://yoursite.com/tags/西瓜书/"}]},{"title":"Hello World","slug":"hello-world","date":"2018-10-24T07:44:41.109Z","updated":"2019-07-16T12:01:44.615Z","comments":true,"path":"2018/10/24/hello-world/","link":"","permalink":"http://yoursite.com/2018/10/24/hello-world/","excerpt":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub.","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[],"tags":[]}]}